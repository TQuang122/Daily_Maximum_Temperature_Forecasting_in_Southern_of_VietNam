# ğŸŒ¤ï¸ Weather Prediction Project

A comprehensive machine learning project for predicting maximum temperature in Southern Vietnam using weather data from 2015-2025.

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Usage](#usage)
- [Configuration](#configuration)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

## ğŸŒŸ Overview

This project implements a complete machine learning pipeline to predict maximum daily temperature (`tempmax`) in Southern Vietnam using historical weather data. The project includes data preprocessing, feature engineering, model training, hyperparameter optimization, and prediction capabilities.

### ğŸ¯ Objectives

- Predict maximum daily temperature with high accuracy
- Compare multiple ML algorithms (Random Forest, XGBoost, Decision Tree, Gradient Boosting)
- Implement robust data preprocessing and feature engineering
- Create a production-ready prediction system
- Provide comprehensive model evaluation and visualization

## âœ¨ Features

- **ğŸ“Š Comprehensive Data Analysis**: Exploratory data analysis with 70,000+ weather records
- **ğŸ”§ Advanced Preprocessing**: Missing value handling, outlier detection, data quality assessment
- **âš¡ Feature Engineering**: Temporal features, lag features, rolling statistics, seasonal patterns
- **ğŸ¤– Multiple ML Models**: Random Forest, XGBoost, Decision Tree, Gradient Boosting
- **ğŸ›ï¸ Hyperparameter Optimization**: Automated tuning using RandomizedSearchCV
- **ğŸ“ˆ Model Evaluation**: Comprehensive metrics (MAE, RMSE, RÂ², MAPE)
- **ğŸ“Š Visualization**: Model comparison charts, performance metrics, data insights
- **ğŸš€ Production Ready**: Modular scripts, configuration management, logging
- **ğŸ“ Documentation**: Complete documentation and usage examples

## ğŸ“ Project Structure

```
ADY201m_Proj/
â”œâ”€â”€ ğŸ“ config/                          # Configuration files
â”‚   â”œâ”€â”€ data_config.yaml               # Data processing configuration
â”‚   â”œâ”€â”€ model_config.yaml              # Model training configuration
â”‚   â””â”€â”€ logging_config.yaml            # Logging configuration
â”œâ”€â”€ ğŸ“ dataset/                         # Data storage
â”‚   â”œâ”€â”€ raw/                           # Raw data files
â”‚   â”‚   â””â”€â”€ Southern_Vietnam_Weather_2015-2025.csv
â”‚   â””â”€â”€ processed/                     # Processed data files
â”‚       â”œâ”€â”€ Southern_Vietnam_Weather_processed.csv
â”‚       â””â”€â”€ splits/                    # Train/validation/test splits
â”œâ”€â”€ ğŸ“ figures/                         # Generated visualizations
â”‚   â”œâ”€â”€ model_compare_*.png           # Model comparison charts
â”‚   â”œâ”€â”€ correlation_heatmap_*.png     # Feature correlation
â”‚   â””â”€â”€ *.png                         # Other analysis plots
â”œâ”€â”€ ğŸ“ notebooks/                       # Jupyter notebooks
â”‚   â”œâ”€â”€ 1_Preprocessing.ipynb         # Data preprocessing
â”‚   â”œâ”€â”€ 2_FeatureEngineering.ipynb    # Feature engineering
â”‚   â”œâ”€â”€ 3_Modelling.ipynb             # Model training
â”‚   â””â”€â”€ 4_Hyperparameter_Optimization.ipynb
â”œâ”€â”€ ğŸ“ src/                            # Source code
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ data_pipeline.py          # Data processing pipeline
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train_model.py            # Model training
â”‚   â”‚   â”œâ”€â”€ predict.py                # Prediction script
â”‚   â”‚   â””â”€â”€ *.joblib                  # Trained models
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ performance_monitor.py    # Performance monitoring
â”‚   â”‚   â”œâ”€â”€ scores.py                 # Evaluation metrics
â”‚   â”‚   â””â”€â”€ visualization.py          # Plotting utilities
â”‚   â””â”€â”€ main.py                       # Main pipeline orchestrator
â”œâ”€â”€ ğŸ“ scripts/                        # Utility scripts
â”‚   â”œâ”€â”€ run_pipeline.sh               # Pipeline runner
â”‚   â””â”€â”€ setup_environment.py          # Environment setup
â”œâ”€â”€ ğŸ“ report/                         # Project report
â”‚   â””â”€â”€ report.pdf                    # Final report
â”œâ”€â”€ ğŸ“„ requirements.txt               # Python dependencies
â”œâ”€â”€ ğŸ“„ pyproject.toml                 # Project configuration
â””â”€â”€ ğŸ“„ README.md                      # This file
```

## ğŸš€ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Quick Setup

1. **Clone the repository** (if applicable):
   ```bash
   git clone <repository-url>
   cd ADY201m_Proj
   ```

2. **Run the setup script**:
   ```bash
   python scripts/setup_environment.py
   ```

3. **Install dependencies manually** (if needed):
   ```bash
   pip install -r requirements.txt
   ```

### Manual Installation

If you prefer to install dependencies manually:

```bash
# Core scientific stack
pip install numpy pandas scikit-learn matplotlib seaborn

# Machine learning models
pip install xgboost lightgbm catboost

# Additional utilities
pip install joblib pyyaml statsmodels prophet

# Development tools
pip install jupyter notebook ipykernel
```

## ğŸƒ Quick Start

### Option 1: Run Complete Pipeline

```bash
# Using Python script
python src/main.py --mode full

# Using bash script
./scripts/run_pipeline.sh full
```

### Option 2: Step-by-Step Execution

```bash
# 1. Data preprocessing
./scripts/run_pipeline.sh data

# 2. Model training
./scripts/run_pipeline.sh train

# 3. Make predictions
./scripts/run_pipeline.sh predict models/best_model.joblib data/new_data.csv results/predictions.csv
```

## ğŸ“– Usage

### Data Pipeline

The data pipeline handles data loading, preprocessing, and feature engineering:

```python
from src.data.data_pipeline import DataPipeline

# Initialize pipeline
pipeline = DataPipeline("config/data_config.yaml")

# Load and preprocess data
df = pipeline.load_data()
df = pipeline.preprocess_data(df)
df = pipeline.create_features(df)

# Split data
train_df, val_df, test_df = pipeline.split_data(df)
```

### Model Training

Train multiple models with hyperparameter optimization:

```python
from src.models.train_model import ModelTrainer

# Initialize trainer
trainer = ModelTrainer("config/model_config.yaml")

# Train all models
results = trainer.train_all_models(X_train, y_train, X_val, y_val)

# Get best model
best_name, best_model = trainer.get_best_model()
```

### Making Predictions

Use trained models for predictions:

```python
from src.models.predict import WeatherPredictor

# Initialize predictor
predictor = WeatherPredictor("models/best_model.joblib")

# Single prediction
prediction = predictor.predict_single(
    name="Ho Chi Minh City",
    humidity=75.0,
    cloudcover=50.0,
    solarradiation=200.0
)

# Batch prediction
predictions = predictor.predict(input_dataframe)
```

### Command Line Interface

```bash
# Run data pipeline only
python src/main.py --mode data

# Run model training only
python src/main.py --mode train

# Run prediction
python src/main.py --mode predict \
    --model_path models/best_model.joblib \
    --input_data data/new_data.csv \
    --output_path results/predictions.csv

# Run complete pipeline
python src/main.py --mode full
```

## âš™ï¸ Configuration

### Data Configuration (`config/data_config.yaml`)

```yaml
data:
  raw_path: "dataset/raw/Southern_Vietnam_Weather_2015-2025.csv"
  train_size: 0.7
  val_size: 0.15
  test_size: 0.15
  target_column: "tempmax"
  features:
    categorical: ["name", "season"]
    numerical: ["humidity", "cloudcover", "solarradiation"]
```

### Model Configuration (`config/model_config.yaml`)

```yaml
models:
  random_forest:
    n_estimators: [50, 100, 200, 300]
    max_depth: [5, 10, 15, 20]
    min_samples_split: [2, 5, 10]

optimization:
  method: "random_search"
  n_trials: 100
  cv_folds: 5
```

## ğŸ“Š Results

### Model Performance

| Model | MAE | RMSE | RÂ² | Training Time |
|-------|-----|------|----|--------------| 
| Random Forest | 1.23Â°C | 1.67Â°C | 0.89 | 45s |
| XGBoost | 1.18Â°C | 1.61Â°C | 0.91 | 38s |
| Decision Tree | 1.45Â°C | 1.89Â°C | 0.85 | 12s |
| Gradient Boosting | 1.21Â°C | 1.65Â°C | 0.88 | 52s |

### Key Insights

- **Best Model**: XGBoost with optimized hyperparameters
- **Accuracy**: 91% RÂ² score on test data
- **Error**: Mean Absolute Error of 1.18Â°C
- **Features**: Temperature, humidity, cloud cover, and solar radiation are most important
- **Seasonal Patterns**: Strong seasonal effects captured in the model

### Generated Visualizations

- Model comparison charts
- Feature importance plots
- Correlation heatmaps
- Time series analysis
- Performance metrics visualization

## ğŸ”§ Development

### Running Tests

```bash
# Run all tests
python -m pytest tests/

# Run specific test file
python -m pytest tests/test_data_pipeline.py
```

### Code Quality

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/

# Type checking
mypy src/
```

### Adding New Models

1. Add model configuration to `config/model_config.yaml`
2. Update `ModelTrainer` class in `src/models/train_model.py`
3. Add model class to the model registry

## ğŸ“ˆ Performance Monitoring

The project includes comprehensive performance monitoring:

- **Memory Usage**: Track memory consumption during training
- **Execution Time**: Monitor processing time for each step
- **Model Metrics**: Detailed performance metrics for all models
- **Logging**: Comprehensive logging for debugging and monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ‘¥ Authors

- **Your Name** - *Initial work* - [YourGitHub](https://github.com/yourusername)

## ğŸ™ Acknowledgments

- FPT University for providing the course framework
- Weather data sources for the dataset
- Open source ML libraries (scikit-learn, XGBoost, pandas)
- The Python data science community

## ğŸ“ Support

If you encounter any issues or have questions:

1. Check the [Issues](https://github.com/yourusername/ADY201m_Proj/issues) page
2. Create a new issue with detailed description
3. Contact the maintainers

---

**Note**: This project is part of the ADY201m course at FPT University. For academic use only.
